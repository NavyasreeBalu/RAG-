# Test queries for RAG evaluation

TEST_QUERIES = [
    "What is the difference between RAG-Sequence and RAG-Token?",
    "How does FlashAttention use tiling to reduce memory I/O compared to standard attention mechanisms?",
    "What is the impact of the rank r hyperparameter in LoRA?",
    "Describe the \"U-shaped\" performance curve observed in long-context language models.",
    "What are the three stages of RLHF used in InstructGPT?",
    "Which paper first demonstrated emergent abilities in large language models, and what does \"emergent\" mean in this context?",
    "How did the Transformer architecture eliminate recurrence when modeling long-range dependencies?",
    "Why is LLaMA particularly important for enterprise and on-premise deployment of large language models?",
    "Compare the bidirectional attention mechanism in BERT with the unidirectional attention in GPT-style models"
]
